{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hanli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "\n",
    "ban_wds = ['anal',\n",
    " 'anus',\n",
    " 'arse',\n",
    " 'ass',\n",
    " 'ballsack',\n",
    " 'balls',\n",
    " 'bastard',\n",
    " 'bitch',\n",
    " 'biatch',\n",
    " 'bloody',\n",
    " 'blowjob',\n",
    " 'blow job',\n",
    " 'bollock',\n",
    " 'bollok',\n",
    " 'boner',\n",
    " 'boob',\n",
    " 'bugger',\n",
    " 'bum',\n",
    " 'butt',\n",
    " 'buttplug',\n",
    " 'clitoris',\n",
    " 'cock',\n",
    " 'coon',\n",
    " 'crap',\n",
    " 'cunt',\n",
    " 'damn',\n",
    " 'dick',\n",
    " 'dildo',\n",
    " 'dyke',\n",
    " 'fag',\n",
    " 'feck',\n",
    " 'fellate',\n",
    " 'fellatio',\n",
    " 'felching',\n",
    " 'fuck',\n",
    " 'f u c k',\n",
    " 'fudgepacker',\n",
    " 'fudge packer',\n",
    " 'flange',\n",
    " 'Goddamn',\n",
    " 'God damn',\n",
    " 'hell',\n",
    " 'homo',\n",
    " 'jerk',\n",
    " 'jizz',\n",
    " 'knobend',\n",
    " 'knob end',\n",
    " 'labia',\n",
    " 'lmao',\n",
    " 'lmfao',\n",
    " 'muff',\n",
    " 'nigger',\n",
    " 'nigga',\n",
    " 'omg',\n",
    " 'penis',\n",
    " 'piss',\n",
    " 'poop',\n",
    " 'prick',\n",
    " 'pube',\n",
    " 'pussy',\n",
    " 'queer',\n",
    " 'scrotum',\n",
    " 'sex',\n",
    " 'shit',\n",
    " 's hit',\n",
    " 'sh1t',\n",
    " 'slut',\n",
    " 'smegma',\n",
    " 'spunk',\n",
    " 'tit',\n",
    " 'tosser',\n",
    " 'turd',\n",
    " 'twat',\n",
    " 'vagina',\n",
    " 'wank',\n",
    " 'whore',\n",
    " 'wtf',\n",
    " 'sucker',\n",
    " 'pornhub',\n",
    " 'xvideo',\n",
    " 'assfuck',\n",
    " 'chinkchong',\n",
    " 'nigger',\n",
    " 'chinkn',\n",
    " 'negro',\n",
    " 'smalleye',\n",
    " 'porn',\n",
    " 'pron','nazi']\n",
    "#Filter function\n",
    "\n",
    "def word_dect(text):\n",
    "    text_cln = text.lower()\n",
    "    text_cln = re.sub('[^A-Za-z0-9]+', ' ', text) #word tokenization and preporcessing\n",
    "    text_cln = re.sub(r'[^\\w\\s]', '', text_cln)\n",
    "    tokens = nltk.word_tokenize(text_cln)\n",
    "    if len(tokens) > 2: # check if the number of tokens is larger than 2\n",
    "        return False\n",
    "    for i in tokens:\n",
    "        if i in ban_wds:\n",
    "            return False\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "    return True\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dect('sucker,leooh,leoohleoohleoohleoohleoohleooh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mini Bert model and tokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/bert-tiny-finetuned-sms-spam-detection\",\n",
    "                                                           num_labels=2, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/bert-tiny-finetuned-sms-spam-detection\")\n",
    "\n",
    "# Example input sentences\n",
    "input_sentences = [\"I love pizza\"]\n",
    "\n",
    "# Tokenize input sentences\n",
    "input_tokens = tokenizer.batch_encode_plus(input_sentences, return_tensors='pt', padding=True)\n",
    "\n",
    "# Run input through Mini Bert model and get attention weights for all layers\n",
    "output = model(input_tokens['input_ids'], attention_mask=input_tokens['attention_mask'], output_attentions=True)\n",
    "attention_weights = output.attentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hanli/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/hanli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSequenceClassification\n",
    "import torch\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    # Lowercase the text\n",
    "    text = str(text).lower()\n",
    "\n",
    "    # Remove special characters and symbols\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Remove \\n symbol\n",
    "    text = re.sub(r'\\n','',text )\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "def spam_dect(text):\n",
    "    # Load the saved model state dictionary\n",
    "    path = 'to_use.pth'\n",
    "    state_dict = torch.load(path)\n",
    "\n",
    "    # Instantiate the model class and load the saved state dictionary\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"mrm8488/bert-tiny-finetuned-sms-spam-detection\",num_labels = 2)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "        # Move the model to the device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    # Tokenize the text\n",
    "    text = preprocess(text)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/bert-tiny-finetuned-sms-spam-detection\")\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    inputs.to(device)\n",
    "    input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)\n",
    "\n",
    "    # Pass the input through the model to get the logits\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs,output_attentions=True)\n",
    "        logits = outputs.logits\n",
    "        attentions = outputs.attentions[0]\n",
    "    avg_attentions = torch.mean(attentions, dim=1)[0]\n",
    "\n",
    "    # Get the predicted label\n",
    "    _, predicted_label = torch.max(logits, dim=1)\n",
    "\n",
    "    # Get the important words\n",
    "    important_words = []\n",
    "    for i in torch.topk(avg_attentions, k=3).indices:\n",
    "        important_words.append(tokenizer.decode(input_ids[0][i].tolist()))\n",
    "\n",
    "    # Print the percentage of label 1\n",
    "    label_1_percentage = torch.softmax(logits, dim=1)[0][1].item() * 100 \n",
    "    return \"RESULT: Your message is \" + str(round(label_1_percentage,2))+ \" percent chance to be a scam. \" + str(important_words[:3]) + ' are the word-combinations that contributing the most to the prediction.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"RESULT: Your message is 0.01 percent chance to be a scam. ['home hey yuki', 'hey home call', 'home hey yuki'] are the word-combinations that contributing the most to the prediction.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_dect(\"Hey yuki, call me when you got home\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
